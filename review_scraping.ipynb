{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import time\n",
    "import pprint\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# base url\n",
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "\n",
    "# getting the url\n",
    "driver.get(base_url)\n",
    "\n",
    "# scrolling to and clicking the button to get 100 reviews per page\n",
    "element_100 = driver.find_element(By.XPATH, '//*[@id=\"main\"]/section[3]/div[1]/article/div[1]/div[2]/form/ul/li[4]/label')\n",
    "driver.execute_script(\"arguments[0].scrollIntoView({'block':'center', 'inline':'center'});\", element_100)\n",
    "element_100.click()\n",
    "\n",
    "# finding the total number of pages\n",
    "num_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/section[3]/div[1]/div/article/ul/li[8]/a')\n",
    "driver.execute_script(\"arguments[0].scrollIntoView({'block':'center', 'inline':'center'});\", num_element)\n",
    "num_pages = int(num_element.text)\n",
    "\n",
    "\n",
    "# https://www.airlinequality.com/airline-reviews/british-airways/page/1/?sortby=post_date%3ADesc&pagesize=100\n",
    "# url for paginated data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page  1\n",
      "scraping page  2\n",
      "scraping page  3\n",
      "scraping page  4\n",
      "scraping page  5\n",
      "scraping page  6\n",
      "scraping page  7\n",
      "scraping page  8\n",
      "scraping page  9\n",
      "scraping page  10\n",
      "scraping page  11\n",
      "scraping page  12\n",
      "scraping page  13\n",
      "scraping page  14\n",
      "scraping page  15\n",
      "scraping page  16\n",
      "scraping page  17\n",
      "scraping page  18\n",
      "scraping page  19\n",
      "scraping page  20\n",
      "scraping page  21\n",
      "scraping page  22\n",
      "scraping page  23\n",
      "scraping page  24\n",
      "scraping page  25\n",
      "scraping page  26\n",
      "scraping page  27\n",
      "scraping page  28\n",
      "scraping page  29\n",
      "scraping page  30\n",
      "scraping page  31\n",
      "scraping page  32\n",
      "scraping page  33\n",
      "scraping page  34\n",
      "scraping page  35\n",
      "scraping page  36\n",
      "scraping page  37\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "ratings = []\n",
    "authors = []\n",
    "countries = []\n",
    "dates = []\n",
    "\n",
    "for i in range(1, num_pages+1):\n",
    "        \n",
    "        print('scraping page ',i)\n",
    "\n",
    "        # url for paginated data\n",
    "        url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize=100\"\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        all_revs = soup.find_all('div', class_ = 'text_content')\n",
    "        all_ratings = soup.find_all('article', attrs={\"itemprop\":\"review\"})\n",
    "        all_authors = soup.find_all('span', attrs={\"itemprop\":\"name\"})\n",
    "        all_countries = soup.find_all('h3', class_=\"text_sub_header userStatusWrapper\")\n",
    "        all_dates = soup.find_all('time', attrs={\"itemprop\":\"datePublished\"})\n",
    "        \n",
    "        for rev in all_revs:\n",
    "                        reviews.append(rev.text)\n",
    "\n",
    "        for rate in all_ratings:\n",
    "                        temp_rate = rate.find(\"div\", class_ = 'rating-10').text\n",
    "                        temp_rate = temp_rate.replace('\\n', '')\n",
    "                        \n",
    "                        try:\n",
    "                                temp_rate = temp_rate.split('/')\n",
    "                                ratings.append(temp_rate[0])\n",
    "\n",
    "                        except:\n",
    "                                ratings.append(temp_rate)\n",
    "\n",
    "        for auth in all_authors:\n",
    "                        authors.append(auth.text)\n",
    "\n",
    "        for count in all_countries:\n",
    "                temp = re.findall(r'\\(.*?\\)', count.text)\n",
    "                try:\n",
    "                        temp = temp[0].strip('(')\n",
    "                        temp = temp.strip(')')\n",
    "                        countries.append(temp)\n",
    "                except:\n",
    "                        countries.append('NA')\n",
    "        \n",
    "        for date in all_dates:\n",
    "                        dates.append(date['datetime'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3695"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a mongo client\n",
    "ba_client = MongoClient()\n",
    "\n",
    "# creating a database\n",
    "ba_database = ba_client['ba_database']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a collection\n",
    "ba_col = ba_database['ba_review_col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserting reviews to mongoDB\n",
    "ids = []\n",
    "for i in range(len(reviews)):\n",
    "        review = {\n",
    "                '_id':i,\n",
    "                'author':authors[i],\n",
    "                'country':countries[i],\n",
    "                'published':dates[i],\n",
    "                'rating':ratings[i],\n",
    "                'review':reviews[i]\n",
    "        }\n",
    "\n",
    "        ids.append(ba_col.insert_one(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>country</th>\n",
       "      <th>published</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graeme Boothman</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2023-11-08</td>\n",
       "      <td>8</td>\n",
       "      <td>✅ Trip Verified |  Booked online months ago an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R Vines</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2023-11-07</td>\n",
       "      <td>7</td>\n",
       "      <td>✅ Trip Verified |  The flight was on time. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Massimo Tricca</td>\n",
       "      <td>Italy</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>2</td>\n",
       "      <td>Not Verified |  Angry, disappointed, and unsat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J Kaye</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>3</td>\n",
       "      <td>✅ Trip Verified |  As an infrequent flyer, Bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M Collie</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>2023-11-04</td>\n",
       "      <td>8</td>\n",
       "      <td>Not Verified |  A totally unremarkable flight,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author         country   published rating  \\\n",
       "0  Graeme Boothman  United Kingdom  2023-11-08      8   \n",
       "1          R Vines  United Kingdom  2023-11-07      7   \n",
       "2   Massimo Tricca           Italy  2023-11-05      2   \n",
       "3           J Kaye  United Kingdom  2023-11-05      3   \n",
       "4         M Collie         Ireland  2023-11-04      8   \n",
       "\n",
       "                                              review  \n",
       "0  ✅ Trip Verified |  Booked online months ago an...  \n",
       "1  ✅ Trip Verified |  The flight was on time. The...  \n",
       "2  Not Verified |  Angry, disappointed, and unsat...  \n",
       "3  ✅ Trip Verified |  As an infrequent flyer, Bri...  \n",
       "4  Not Verified |  A totally unremarkable flight,...  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['author'] = authors\n",
    "df['country'] = countries\n",
    "df['published'] = dates\n",
    "df['rating'] = ratings\n",
    "df['review'] = reviews\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:/Users/snkri/OneDrive/Desktop/virtual_interns/BA_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f7924c4c56b083e0e50eadfe7ef592a7a8ef70df33a0047f82280e6be1afe15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

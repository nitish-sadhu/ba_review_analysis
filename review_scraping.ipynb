{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import time\n",
    "import pprint\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# base url\n",
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "\n",
    "# getting the url\n",
    "driver.get(base_url)\n",
    "\n",
    "# scrolling to and clicking the button to get 100 reviews per page\n",
    "element_100 = driver.find_element(By.XPATH, '//*[@id=\"main\"]/section[3]/div[1]/article/div[1]/div[2]/form/ul/li[4]/label')\n",
    "driver.execute_script(\"arguments[0].scrollIntoView({'block':'center', 'inline':'center'});\", element_100)\n",
    "element_100.click()\n",
    "\n",
    "# finding the total number of pages\n",
    "num_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/section[3]/div[1]/div/article/ul/li[8]/a')\n",
    "driver.execute_script(\"arguments[0].scrollIntoView({'block':'center', 'inline':'center'});\", num_element)\n",
    "num_pages = int(num_element.text)\n",
    "\n",
    "\n",
    "# https://www.airlinequality.com/airline-reviews/british-airways/page/1/?sortby=post_date%3ADesc&pagesize=100\n",
    "# url for paginated data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page  1\n",
      "scraping page  2\n",
      "scraping page  3\n",
      "scraping page  4\n",
      "scraping page  5\n",
      "scraping page  6\n",
      "scraping page  7\n",
      "scraping page  8\n",
      "scraping page  9\n",
      "scraping page  10\n",
      "scraping page  11\n",
      "scraping page  12\n",
      "scraping page  13\n",
      "scraping page  14\n",
      "scraping page  15\n",
      "scraping page  16\n",
      "scraping page  17\n",
      "scraping page  18\n",
      "scraping page  19\n",
      "scraping page  20\n",
      "scraping page  21\n",
      "scraping page  22\n",
      "scraping page  23\n",
      "scraping page  24\n",
      "scraping page  25\n",
      "scraping page  26\n",
      "scraping page  27\n",
      "scraping page  28\n",
      "scraping page  29\n",
      "scraping page  30\n",
      "scraping page  31\n",
      "scraping page  32\n",
      "scraping page  33\n",
      "scraping page  34\n",
      "scraping page  35\n",
      "scraping page  36\n",
      "scraping page  37\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "\n",
    "for i in range(1, num_pages+1):\n",
    "        \n",
    "        print('scraping page ',i)\n",
    "\n",
    "        # url for paginated data\n",
    "        url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize=100\"\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        all_revs = soup.find_all('div', class_ = 'text_content')\n",
    "        \n",
    "        for rev in all_revs:\n",
    "                        reviews.append(rev.text)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3694\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>✅ Trip Verified |  The flight was on time. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Verified |  Angry, disappointed, and unsat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified |  As an infrequent flyer, Bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not Verified |  A totally unremarkable flight,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified |   1. Ground crew in Heathrow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  ✅ Trip Verified |  The flight was on time. The...\n",
       "1  Not Verified |  Angry, disappointed, and unsat...\n",
       "2  ✅ Trip Verified |  As an infrequent flyer, Bri...\n",
       "3  Not Verified |  A totally unremarkable flight,...\n",
       "4  ✅ Trip Verified |   1. Ground crew in Heathrow..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:/Users/snkri/OneDrive/Desktop/virtual_interns/BA_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a connection with postgres\n",
    "conn = psycopg2.connect(\n",
    "        database = 'ba_reviews',\n",
    "        user = 'postgres',\n",
    "        password = 'Hemanthkumar#1',\n",
    "        port = '5432',\n",
    "        host = 'localhost'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# creating an engine\n",
    "engine = create_engine('postgresql+psycopg2://postgres:Hemanthkumar#1@localhost:5432/ba_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql('ba_reviews', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('SELECT * FROM ba_reviews')\n",
    "\n",
    "# fetching the table\n",
    "table = cursor.fetchall()\n",
    "\n",
    "test_df = pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not Verified |  Angry, disappointed, and unsatisfied. My route was from London to Atlanta. My suitcase was not boarded, therefore not landed with me. For both comfort and safety reason, a bag always fly with its passenger and that did not happen. Claims and few phone calls were made by desk assistants who answered my questions unprofessionally and miserably. Certainly, I was left with nothing but my backpack which contained not more than few snacks. Neither clothes nor anything else was ever provided as an apology. Meanwhile, I was also told that my bag would have been delivered through the next 24 hours which also did not happen. British Airways is a great airline to fly with but its organization, when it comes to customer service, is poor and uncertain. Still waiting for my bag.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0                                                  1\n",
      "0  0  ✅ Trip Verified |  The flight was on time. The...\n",
      "1  1  Not Verified |  Angry, disappointed, and unsat...\n",
      "2  2  ✅ Trip Verified |  As an infrequent flyer, Bri...\n",
      "3  3  Not Verified |  A totally unremarkable flight,...\n",
      "4  4  ✅ Trip Verified |   1. Ground crew in Heathrow...\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f7924c4c56b083e0e50eadfe7ef592a7a8ef70df33a0047f82280e6be1afe15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
